---
layout: post
title: "朴素贝叶斯"
modified:
categories: [机器学习,算法]
excerpt: 该博客介绍贝叶斯原理，同时介绍算法在实际应用中的使用过程。
tags: [机器学习,算法]
image: 
  feature: so-simple-sample-image-7.jpg
date: 2017-3-8T15:39:55-04:00
---

对于分类而言，使用概率有时要比使用绝对评判有效。贝叶斯概率及准则提供了一种利用已知值来估计未知概率的有效方法。

## 原理及推导过程

必要的，先回顾概率论入门计算公式：

**乘法：**

$$
P(AB) = P(A)P(B|A)  
$$

 这里不要和独立概率的乘法混淆，只有当A、B相互独立，才有 P(AB) = P(A)P(B)

**条件概率：**

$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$

---

前提条件：现在拥有带有标签的数据（或者说已经分类好的数据）集合S，其中类别 Ci ; 给定观测数据 **W**，**W**是一个特征向量，即 W = [w1,w2,w3...wn] ;<br><br>
求解：某个类别中能够生成该特征的概率 $$ P(Ci | W)  $$

---

根据条件概率计算公式可以得出，这也是贝叶斯定理的表达式：

$$
P(C_i|W) = \frac{P(C_i)P(W|C_i)}{P(W)}
$$

- 注意了，由于给定观测数据**W**，这个观测数据已经发生了，即其概率 ：

	$$
	P(W) = 1
	$$

- 同时朴素贝叶斯假设的条件是：每个特征之间相互独立（条件独立性假设），那么有：

	$$
	P(W|C_i) = P(w1|C_i)P(w2|C_i)...P(wn|C_i)
	$$
	
上式理解为：Ci 表征为 类i 中所包含的所有特征总数；wj 表征为某个特征出现的总数。即有：

$$
P(W_j|C_i) =\frac{W_j的个数}{类C_i中所有特征的个数}
$$

为了在计算时不出现累积概率值为0的情况，可以使用取对数的方式，即 使用$$ln(a*b) = ln(a)+ln(b)$$ 的方法避免。

---

## 垃圾邮件概率计算过程
 
假设有训练样本（邮件）集N个，其中非垃圾 i 个，垃圾 j 个。

**算法过程：**

> 
1. 读取所有样本集中的字符串并切分单词，以在样本中出现的单词集合构建特征（单词）集合，即一个超长一维向两
2. 使用留存交叉策略（即在所给样本集中随机抽取K个样本用于测试模型）
3. 对每个样本（邮件），分配一个与1中等长的列表，然后利用词袋模型（如果样本的特征在集合中出现，这对应的权值加一）生成特征向两，用于表示该样本。
4. 利用 append() 函数构建多维特征矩阵，每行代表一个样本特征。
5. 依照公式计算 P(Ci) 和  P(Wj\|Ci)，这里对针对训练集进行计算，计算完之后就是我们需要的模型了，模型中记录每个特征对应在每个类别中的权重，这样在给定特征向量时，就可以通过该模型估计生成概率
6. 利用贝叶斯公式计算生成概率，完成概率预测

为方便4理解，这里举个小例子：

如所有样本中出现的单词为：[wo shi hao ren ma lao die sha bi ]<br>
那么对于某个样本出现如下：[wo shi sha bi] <br>
则对应的特征向两表示如下：[1,1,0,0,0,0,0,1,1]
 >